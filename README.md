<h1 align="center">Hi, I am Jiawei Wang 👋</h1>
<p align="left"> <img src="https://komarev.com/ghpvc/?username=JarvisUSTC&label=Visitors&color=blue&style=plastic" alt="JarvisUSTC" /></p>
<p>Homepage: https://jarvisustc.github.io/</p>

<p>I am a fourth-year Ph.D. student in the joint program between the <a href="https://ustc.edu.cn/">University of Science and Technology of China (USTC)</a> and <a href="https://www.microsoft.com/en-us/research/lab/microsoft-research-asia/">Microsoft Research Asia (MSRA)</a>, co-supervised by <a href="https://www.microsoft.com/en-us/research/people/qianghuo/">Prof. Qiang Huo</a> at MSRA and <a href="http://staff.ustc.edu.cn/~jundu/">Prof. Jun Du</a> at USTC. My Ph.D. research focuses on Document Intelligence (including OCR, document layout analysis, and document understanding) and Large Language Models (including MLLM, Agent, and RAG). Prior to this, I received my B.S. degree from the School of the Gifted Young (a.k.a. 少年班) at the University of Science and Technology of China in 2021, majoring in Computer Science.</p>

<p>During my Ph.D. studies, I gained valuable industry experience through internships at MSRA, DeepSeek, and ByteDance. At DeepSeek, I contributed to the development of DeepSeek VL2, and DeepSeek V3. My internship at MSRA involved working on the <a href="https://learn.microsoft.com/en-us/azure/ai-services/computer-vision/overview-ocr">Microsoft OneOCR project</a> and the <a href="https://azure.microsoft.com/en-us/products/ai-services/ai-document-intelligence">Microsoft Document Intelligence project</a> under the guidance of Researcher <a href="https://www.microsoft.com/en-us/research/people/qianghuo/">Qiang Huo</a> and <a href="https://scholar.google.com/citations?user=cUfOZxQAAAAJ&amp;hl=en">Lei Sun</a>. Most recently, I began an internship with the <a href="https://seed.bytedance.com/en/">ByteDance Seed team</a>, where I am working on LLM/MLLM Agent projects. I have published over 10 papers at top-tier international AI journals and conferences.</p>

<p><strong>I am currently seeking full-time job opportunities.</strong> If you are interested in my resume, please feel free to email me at <a href="mailto:jarvisustc@gmail.com">jarvisustc@gmail.com</a>. I am currently based in Beijing, China. If you would like to have a coffee chat, please feel free to reach out! ☕😊✨</p>

<h1 id="-news">🔥 News</h1>

<ul>
<li><b>2025.09</b>: 🔥 We introduce <a href="https://huggingface.co/papers/2509.09265"><b>EMPG</b></a>: a new framework that solves the credit assignment bottleneck in long-horizon agent training by fixing a fundamental flaw in policy gradients. 🚧 Please find all details in our <a href="https://empgseed-seed.github.io/"><b>project page</b></a>.</li>
<li><b>2025.09</b>: 🎉 We're excited to have contributed to <a href="https://mcpmark.ai/"><b>MCP Mark</b></a>, a solid benchmark for stress-testing comprehensive MCP use. We have open-sourced all details in <a href="https://github.com/eval-sys/mcpmark/">Github</a>. Welcome to join us!</li>
<li><b>2025.08</b>: 🔥 We introduce <a href="https://arxiv.org/pdf/2508.07999"><b>WideSearch</b></a>: a new benchmark to test if AI agents can handle large-scale, repetitive information gathering — the real bottleneck in productivity. 🚧 Please find all details in our <a href="https://widesearch-seed.github.io/"><b>project page</b></a>.</li>
<li><b>2025.06</b>: 🎉 Our paper on VLM robustness, "<a href="https://arxiv.org/abs/2504.01308"><b>Safeguarding Vision-Language Models: Mitigating Vulnerabilities to Gaussian Noise in Perturbation-based Attacks</b></a>," has been accepted by ICCV 2025! See you in Hawaii!</li>
<li><b>2025.05</b>: 🔥 Our latest research, <a href="https://arxiv.org/abs/2505.19630"><b>DoctorAgent-RL: A Multi-Agent Collaborative Reinforcement Learning System for Multi-Turn Clinical Dialogue</b></a>, is now available. This work highlights a crucial principle in Human-Agent Interaction: Agents must proactively request necessary information to excel, as humans may not always volunteer it. This "Agent-must-ask" paradigm is central to DoctorAgent-RL's ability to facilitate better task completion in complex multi-turn dialogues. </li>
<li><b>2025.04</b>: 🎉 Thrilled to kick off my new internship with the ByteDance Seed Team!</li>
<li><b>2025.04</b>: 🔥 Our latest work on VLM robustness, "<a href="https://arxiv.org/abs/2504.01308"><b>Safeguarding Vision-Language Models: Mitigating Vulnerabilities to Gaussian Noise in Perturbation-based Attacks</b></a>," has been released on Arxiv. We've open-sourced the <a href="https://huggingface.co/datasets/Jarvis1111/RobustVLGuard"><b>Robust-VLGuard dataset</b></a> and <a href="https://github.com/JarvisUSTC/DiffPure-RobustVLM"><b>DiffPure-VLM defense</b></a>.</li>
<li><b>2025.03</b>: 🎉 Our paper "<a href="https://arxiv.org/abs/2503.15893"><b>UniHDSA: A Unified Relation Prediction Approach for Hierarchical Document Structure Analysis</b></a>" has been accepted by Pattern Recognition Journal!</li>
<li><b>2024.12</b>: 💻 We've launched a new GitHub project: <a href="https://github.com/JarvisUSTC/Awesome-Multimodal-RAG"><b>Awesome-Multimodal-RAG</b></a>! Check out the latest in multimodal RAG and contribute!</li>
<li><b>2024.12</b>: 🤝 We're excited to have contributed to <a href="https://github.com/deepseek-ai/DeepSeek-VL2"><b>DeepSeek-VL2</b></a>, an advanced Vision-Language Model with strong performance and fewer parameters.</li>
</ul>
<details>
<summary> 🔥 More News</summary>
<ul>
<li><b>2024.08-09</b>: 🗣️ Presented DLAFormer and DRFormer at ICDAR in Athens! Photos can be found <a href="https://photos.app.goo.gl/8aw4mYxDUtA5ELuJ6">here</a>. A memorable experience meeting colleagues and exploring the city.</li>
<li><b>2024.08</b>: ✍️ The complete version of DLAFormer, titled "<a href="https://arxiv.org/abs/2503.15893"><b>UniHDSA: A Unified Relation Prediction Approach for Hierarchical Document Structure Analysis</b></a>", has been submitted to Pattern Recognition Journal.</li>
<li><b>2024.07</b>: 🎉 Our Detect-Order-Construct have been accepted by Pattern Recognition!</li>
<li><b>2024.06</b>: 🗣️ Our DLAFormer, UniVIE, and DRFormer selected for oral presentation at ICDAR 2024!</li>
<li><b>2024.03</b>: 🚀 Azure AI Document Intelligence now supports Hierarchical Document Structure Analysis (HDSA), based on our "<a href="https://arxiv.org/abs/2401.11874"><b>Detect-Order-Construct: A Tree Construction based Approach for Hierarchical Document Structure Analysis</b></a>" paper. Details on <a href="https://arxiv.org/abs/2401.11874">arXiv</a> and the <a href="https://techcommunity.microsoft.com/t5/ai-azure-ai-services-blog/document-intelligence-preview-adds-more-prebuilts-support-for/ba-p/4084608">official announcement</a>.</li>
<li><b>2024.03</b>: 💻 Source code released for our <a href="https://github.com/JarvisUSTC/Language-Enhanced-CLIP-For-Multi-label-Image-Recognition"><b>Language-Enhanced Image New Category Discovery solution</b></a> from the CVPR 2023 HIT Workshop.</li>
<li><b>2024.02</b>: ✍️ Our new work on Document Layout Analysis, <b>DLAFormer: A End-to-End Transformer for Document Layout Analysis</b>, submitted to ICDAR 2024.</li>
<li><b>2024.01</b>: 💡 Introduced <a href="https://arxiv.org/abs/2401.09220"><b>UniVIE: A Unified Label Space Approach to Visual Information Extraction from Form-like Documents</b></a>! Reframing VIE as relation prediction with a unified label space.</li>
<li><b>2024.01</b>: 📄 New technical paper released: <a href="https://arxiv.org/abs/2401.09232"><b>Dynamic Relation Transformer for Contextual Text Block Detection</b></a>!</li>
<li><b>2023.12</b>: 🏆 <a href="https://mp.weixin.qq.com/s/B1r2uJ0bNg_u50vuNI5MPw"><b>2nd Prize</b>, 2023 International Algorithm Case Competition (Visual Prompt Tuning Challenge @ CVPR 2023 HIT Workshop)</a>, <b>200,000 RMB bonus</b>!</li>
<li><b>2023.11</b>: ✍️ Our new progress on Hierarchical Document Structure Analysis submitted to Pattern Recognition Journal.</li>
<li><b>2023.07</b>: 🎉 "<a href="https://arxiv.org/abs/2303.11615"><b>Robust Table Structure Recognition with Dynamic Queries Enhanced Detection Transformer</b></a>" accepted by Pattern Recognition Journal!</li>
<li><b>2023.04</b>: 🎉 Two papers accepted by <b>ICDAR 2023</b>!</li>
<li><b>2023.03</b>: 💡 Proposed a new <a href="https://arxiv.org/abs/2303.11615"><b>Dynamic Queries based Detection Transformer</b></a> for more robust table structure recognition!</li>
<li><b>2022.12</b>: 🏆 <a href="https://mp.weixin.qq.com/s/Au8oRHbX0Ls2gL4WiBNmqw"><b>2nd Prize</b>, 2022 International Algorithm Case Competition (Panoptic Scene Graph Challenge @ ECCV 2022 SenseHuman Workshop)</a>, <b>100,000 RMB bonus</b>!</li>
<li><b>2022.09</b>: 🎉 One paper accepted by <b>ACM MM 2022</b>!</li>
</ul>
</details>

<h1 id="-experiences">💻 Experiences</h1>
<ul>
  <li><strong>2025.4-Now</strong>: Research Intern, Seed Team, <strong>ByteDance</strong> <img src="./images/bytedance-seed.jpg" style="width: 4em;" />, Beijing, China.</li>
  <li><strong>2024.09-2025.03</strong>: Research Intern, Multimodal Interaction Group, <strong>Microsoft Research Asia</strong> <img src="./images/microsoft_logo.svg" style="width: 4em;" />, Beijing, China.</li>
  <li><strong>2024.06-2024.08</strong>: AGI Research Intern, Multimodal LLM Team, <strong>DeepSeek</strong> <img src="./images/deepseek_logo.png" style="width: 4em;" />, Beijing, China.</li>
  <li><strong>2020.09-2024.05</strong>: Research Intern, Multimodal Interaction Group, <strong>Microsoft Research Asia</strong> <img src="./images/microsoft_logo.svg" style="width: 4em;" />, Beijing, China.</li>
</ul>

<h1 id="-educations">📖 Educations</h1>
<ul>
  <li><strong>2021.09-2026.6</strong>: Ph.D. in Information and Communication Engineering, University of Science and Technology of China, Hefei, Anhui, China.</li>
  <li><strong>2017.09-2021.06</strong>: B.S. in the School of the Gifted Young (major in Computer Science), University of Science and Technology of China, Hefei, Anhui, China.</li>
</ul>
<!--
**JarvisUSTC/JarvisUSTC** is a ✨ _special_ ✨ repository because its `README.md` (this file) appears on your GitHub profile.

Here are some ideas to get you started:

- 🔭 I’m currently working on ...
- 🌱 I’m currently learning ...
- 👯 I’m looking to collaborate on ...
- 🤔 I’m looking for help with ...
- 💬 Ask me about ...
- 📫 How to reach me: ...
- 😄 Pronouns: ...
- ⚡ Fun fact: ...
-->
